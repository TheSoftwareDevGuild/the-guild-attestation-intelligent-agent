{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8339467",
   "metadata": {},
   "source": [
    "## DEVFOLIO ONCHAIN CREDENTIALS ATTESTATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b377839",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883c1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc69245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_root_dir():\n",
    "    return \"../../data/eas_attestations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88992b9e",
   "metadata": {},
   "source": [
    "### Refine data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c25e8e",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "devfolio_o_c_a_schema_id=\"0x364a59df1d48d4b6c0f8f0c1176504b252bce5ce57e0d1ca75b1bf70c2f0ec14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57030c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def filter_attestations_by_schema_id(schemaId,fileName=\"devfolio_o_c_a_schema\"):\n",
    "\n",
    "    in_path = f\"{get_data_root_dir()}/enriched_attestation_events.jsonl\"\n",
    "    out_path = f\"{get_data_root_dir()}/filtered_attestation_with_{fileName}.jsonl\"\n",
    "\n",
    "    total = 0\n",
    "    with open(in_path, \"r\") as fin, open(out_path, \"w\") as fout:\n",
    "        for line in fin:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            if obj[\"schema\"]==schemaId:\n",
    "                fout.write(json.dumps(obj) + \"\\n\")\n",
    "                total += 1\n",
    "\n",
    "    print(f\"Found {total} attestations for schema '{schemaId}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73bfa85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1524 attestations for schema '0x364a59df1d48d4b6c0f8f0c1176504b252bce5ce57e0d1ca75b1bf70c2f0ec14'\n"
     ]
    }
   ],
   "source": [
    "filter_attestations_by_schema_id(devfolio_o_c_a_schema_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e39e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cb3346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_data type: <class 'dict'>\n",
      "decoded_data keys (first 20): ['nft_metadata_ipfs_url', 'user_uuid', 'credential_uuid', 'hackathon_uuid', 'user_hackathon_credential_uuid', 'nft_contract_address']\n",
      "decoded_data sample: {'nft_metadata_ipfs_url': 'https://ipfs.io/ipfs/Qmd7FukB2LASSTLLtZw6Lpz2GWfNDKg21ABSpVivG8F5GW', 'user_uuid': 'dd743bbd44274a8a9b65759f287d216d', 'credential_uuid': 'b1426e6a8b7d4867924f588e95eb70b1', 'hackathon_uuid': 'becdb269b9ea4e708c7d96329563e478', 'user_hackathon_credential_uuid': '1821718837164bd184737133bbcdf0a8'}\n",
      "First NFT metadata URL: https://ipfs.io/ipfs/Qmd7FukB2LASSTLLtZw6Lpz2GWfNDKg21ABSpVivG8F5GW\n",
      "\n",
      "First Element Details:\n",
      "----------------------------------------\n",
      "\n",
      "id:\n",
      "0x153909c95a54503a1f3d08dde5a4a4700bdf7e8d6e3360af1df6ce7ad23e8fb6\n",
      "\n",
      "attester:\n",
      "0x3Ce7b2b2a9F3C27aFa6EC511679f606412fb497b\n",
      "\n",
      "recipient:\n",
      "0x3c1763006FcdEa4b467cC8FE9c28Fab664d0F6ED\n",
      "\n",
      "refUID:\n",
      "0x0000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "revocable:\n",
      "True\n",
      "\n",
      "revocationTime:\n",
      "0\n",
      "\n",
      "expirationTime:\n",
      "0\n",
      "\n",
      "schema:\n",
      "0x364a59df1d48d4b6c0f8f0c1176504b252bce5ce57e0d1ca75b1bf70c2f0ec14\n",
      "\n",
      "block_number:\n",
      "296748689\n",
      "\n",
      "blockchain_name:\n",
      "arbitrum\n",
      "\n",
      "decoded_data:\n",
      "{\n",
      "  \"nft_metadata_ipfs_url\": \"https://ipfs.io/ipfs/Qmd7FukB2LASSTLLtZw6Lpz2GWfNDKg21ABSpVivG8F5GW\",\n",
      "  \"user_uuid\": \"dd743bbd44274a8a9b65759f287d216d\",\n",
      "  \"credential_uuid\": \"b1426e6a8b7d4867924f588e95eb70b1\",\n",
      "  \"hackathon_uuid\": \"becdb269b9ea4e708c7d96329563e478\",\n",
      "  \"user_hackathon_credential_uuid\": \"1821718837164bd184737133bbcdf0a8\",\n",
      "  \"nft_contract_address\": \"0xe34494de41383fbad7d1cdba6730d0e943425701\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# get filtered_attestation_with_devfolio_o_c_a_schema.jsonl\n",
    "df = pd.read_json(\"../../data/eas_attestations/filtered_attestation_with_devfolio_o_c_a_schema.jsonl\", lines=True)\n",
    "\n",
    "# Inspect decoded_data structure on the first row\n",
    "first_decoded = df.iloc[0][\"decoded_data\"] if isinstance(df.iloc[0], pd.Series) else df.iloc[0].decoded_data\n",
    "print(\"decoded_data type:\", type(first_decoded))\n",
    "if isinstance(first_decoded, dict):\n",
    "    print(\"decoded_data keys (first 20):\", list(first_decoded.keys())[:20])\n",
    "    print(\"decoded_data sample:\", {k: first_decoded[k] for k in list(first_decoded.keys())[:5]})\n",
    "else:\n",
    "    print(\"decoded_data value:\", first_decoded)\n",
    "\n",
    "# Safely extract nft_metadata_ipfs_url if present in dict\n",
    "def get_first_nft_metadata_url(df):\n",
    "    \"\"\"Print the nft_metadata_ipfs_url from the first row of the dataframe if available\"\"\"\n",
    "    decoded = df.iloc[0][\"decoded_data\"]\n",
    "    first_url = decoded.get(\"nft_metadata_ipfs_url\") if isinstance(decoded, dict) else None\n",
    "    print(f\"First NFT metadata URL: {first_url}\")\n",
    "    return first_url\n",
    "\n",
    "get_first_nft_metadata_url(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cf25737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Element Details:\n",
      "----------------------------------------\n",
      "\n",
      "id:\n",
      "0x153909c95a54503a1f3d08dde5a4a4700bdf7e8d6e3360af1df6ce7ad23e8fb6\n",
      "\n",
      "attester:\n",
      "0x3Ce7b2b2a9F3C27aFa6EC511679f606412fb497b\n",
      "\n",
      "recipient:\n",
      "0x3c1763006FcdEa4b467cC8FE9c28Fab664d0F6ED\n",
      "\n",
      "refUID:\n",
      "0x0000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "revocable:\n",
      "True\n",
      "\n",
      "revocationTime:\n",
      "0\n",
      "\n",
      "expirationTime:\n",
      "0\n",
      "\n",
      "schema:\n",
      "0x364a59df1d48d4b6c0f8f0c1176504b252bce5ce57e0d1ca75b1bf70c2f0ec14\n",
      "\n",
      "block_number:\n",
      "296748689\n",
      "\n",
      "blockchain_name:\n",
      "arbitrum\n",
      "\n",
      "decoded_data:\n",
      "{\n",
      "  \"nft_metadata_ipfs_url\": \"https://ipfs.io/ipfs/Qmd7FukB2LASSTLLtZw6Lpz2GWfNDKg21ABSpVivG8F5GW\",\n",
      "  \"user_uuid\": \"dd743bbd44274a8a9b65759f287d216d\",\n",
      "  \"credential_uuid\": \"b1426e6a8b7d4867924f588e95eb70b1\",\n",
      "  \"hackathon_uuid\": \"becdb269b9ea4e708c7d96329563e478\",\n",
      "  \"user_hackathon_credential_uuid\": \"1821718837164bd184737133bbcdf0a8\",\n",
      "  \"nft_contract_address\": \"0xe34494de41383fbad7d1cdba6730d0e943425701\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# pretty print the first entry:\n",
    "def pretty_print_first_element(df):\n",
    "    \"\"\"Pretty prints the first element of the dataframe\"\"\"\n",
    "    import json\n",
    "    first_element = df.iloc[0]\n",
    "    print(\"\\nFirst Element Details:\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in first_element.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        if key == \"decoded_data\":\n",
    "            print(json.dumps(value, indent=2))\n",
    "        else:\n",
    "            print(value)\n",
    "\n",
    "pretty_print_first_element(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f219a",
   "metadata": {},
   "source": [
    "#### Trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "118581c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove refUID, revocable, revocationTime, expirationTime fields\n",
    "def remove_useless_fields(df):\n",
    "    return df[[\"id\",\"attester\",\"recipient\",\"schema\",\"block_number\",\"blockchain_name\",\"decoded_data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08b1cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Element Details:\n",
      "----------------------------------------\n",
      "\n",
      "id:\n",
      "0x153909c95a54503a1f3d08dde5a4a4700bdf7e8d6e3360af1df6ce7ad23e8fb6\n",
      "\n",
      "attester:\n",
      "0x3Ce7b2b2a9F3C27aFa6EC511679f606412fb497b\n",
      "\n",
      "recipient:\n",
      "0x3c1763006FcdEa4b467cC8FE9c28Fab664d0F6ED\n",
      "\n",
      "schema:\n",
      "0x364a59df1d48d4b6c0f8f0c1176504b252bce5ce57e0d1ca75b1bf70c2f0ec14\n",
      "\n",
      "block_number:\n",
      "296748689\n",
      "\n",
      "blockchain_name:\n",
      "arbitrum\n",
      "\n",
      "decoded_data:\n",
      "{\n",
      "  \"nft_metadata_ipfs_url\": \"https://ipfs.io/ipfs/Qmd7FukB2LASSTLLtZw6Lpz2GWfNDKg21ABSpVivG8F5GW\",\n",
      "  \"user_uuid\": \"dd743bbd44274a8a9b65759f287d216d\",\n",
      "  \"credential_uuid\": \"b1426e6a8b7d4867924f588e95eb70b1\",\n",
      "  \"hackathon_uuid\": \"becdb269b9ea4e708c7d96329563e478\",\n",
      "  \"user_hackathon_credential_uuid\": \"1821718837164bd184737133bbcdf0a8\",\n",
      "  \"nft_contract_address\": \"0xe34494de41383fbad7d1cdba6730d0e943425701\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "df_trimmed=remove_useless_fields(df)\n",
    "pretty_print_first_element(df_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa9cf8",
   "metadata": {},
   "source": [
    "#### Get ipfs hosted json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc1e06ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"project_url\": \"https://devfolio.co/projects/empoweria-e0ee\",\n",
      "  \"attributes\": [\n",
      "    {\n",
      "      \"trait_type\": \"hackathon_name\",\n",
      "      \"value\": \"ETHIndia 2023\"\n",
      "    },\n",
      "    {\n",
      "      \"trait_type\": \"nft_type\",\n",
      "      \"value\": \"BUILDER\"\n",
      "    },\n",
      "    {\n",
      "      \"trait_type\": \"team_name\",\n",
      "      \"value\": \"DEFY\"\n",
      "    },\n",
      "    {\n",
      "      \"trait_type\": \"project_name\",\n",
      "      \"value\": \"Empoweria\"\n",
      "    },\n",
      "    {\n",
      "      \"display_type\": \"date\",\n",
      "      \"trait_type\": \"project_submission_date\",\n",
      "      \"value\": 1702179294391\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "# ipfs link like https://ipfs.io/ipfs/Qmd7FukB2LASSTLLtZw6Lpz2GWfNDKg21ABSpVivG8F5GW contains a json, lets get it\n",
    "def get_ipfs_json(ipfs_link):\n",
    "    \"\"\"Gets JSON data from an IPFS link\n",
    "    \n",
    "    Args:\n",
    "        ipfs_link (str): IPFS link to JSON data\n",
    "        \n",
    "    Returns:\n",
    "        dict: JSON data from IPFS link\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(ipfs_link)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        return response.json()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching IPFS data: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON data: {e}\")\n",
    "        return None\n",
    "    \n",
    "res=get_ipfs_json(\"https://ipfs.io/ipfs/Qmd7FukB2LASSTLLtZw6Lpz2GWfNDKg21ABSpVivG8F5GW\")\n",
    "# print(json.dumps(res, indent=2))\n",
    "\n",
    "def process_ipfs_json(ipfs_json):\n",
    "    # search https://devfolio.co/projects/empoweria-e0ee regexp pattern in the description field\n",
    "    match = re.search(r'(https:\\/\\/devfolio\\.co\\/projects\\/[^\\)\\s]+)', ipfs_json[\"description\"])\n",
    "    project_url = match.group(1) if match else None\n",
    "    return {\n",
    "        \"project_url\": project_url,\n",
    "        \"attributes\": ipfs_json[\"attributes\"]\n",
    "    }\n",
    "\n",
    "res=process_ipfs_json(res)\n",
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1246b522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"url\": \"https://devfolio.co/projects/empoweria-e0ee\",\n",
      "  \"title\": \"Empoweria\",\n",
      "  \"subtitle\": \"A platform that lets the citizens speak without fear and take the community towards development together\",\n",
      "  \"technologies\": [\n",
      "    \"Solidity\",\n",
      "    \"Node.js\",\n",
      "    \"Next.js\",\n",
      "    \"TypeScript\",\n",
      "    \"MongoDB\",\n",
      "    \"mongoose\",\n",
      "    \"Hardhat\"\n",
      "  ],\n",
      "  \"matching_amount_usd\": 109,\n",
      "  \"votes\": 531,\n",
      "  \"quadratic_votes\": 88.183,\n",
      "  \"built_at\": \"ETHIndia 2023 Created on 10th December 2023\",\n",
      "  \"created_on\": \"10th December 2023\",\n",
      "  \"last_edited\": \"10th December 2023\",\n",
      "  \"problem_statement\": \"In the fabric of our communities, a pervasive silence has taken root, stifling the voices of those yearning to speak out due to the paralyzing fear of judgment and reprisal. In response to this, we embark on a transformative mission, introducing a groundbreaking platform designed to empower the common individual to articulate their societal challenges and grievances anonymously. Utilizing anon aadhar, our platform ensures Indian identity verification while preserving the sanctity of user anonymity through a sophisticated Circom extractor, instilling a sense of trust within the community. Beyond mere text, we integrate video calling and chat options powered by interest matching, fostering dynamic conversations among community members. With the secure and decentralized Waku protocol, we create a confidential space where individuals can engage freely, unburdened by societal constraints. This initiative aims to dismantle the barriers of fear, nurturing a more inclusive society where every voice, particularly the common Joe's, resonates and contributes to a collective narrative that embraces diversity and addresses shared challenges. Together, we forge a path towards a society where the strength of community lies not just in unity but in the freedom to express, ensuring that every individual is not merely heard but genuinely listened to, fostering understanding and empathy in the tapestry of our shared human experience.\",\n",
      "  \"challenges\": \"Getting internet connectivity was very difficult so the pace of project slowed down a lot. We built a circom extractor to extract certain regex in our pdf , but we could not build the mechanism to check if the pdf document was tampered or not. We were facing web sockets issue in Waku and huddle, we had to deal with interest matching so that we pair people on based on their interests.\",\n",
      "  \"github_link\": \"https://github.com/PoulavBhowmick03/ethindia\\\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import Dict, Any, List, Optional\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "_PROBLEM_HEADING_RE = re.compile(r'^\\s*the\\s+problem\\b.*\\bsolves\\b', re.I)\n",
    "_CHALLENGE_RE = re.compile(r'^\\s*Challenges', re.I)\n",
    "\n",
    "def extract_problem_section(soup: BeautifulSoup, regex) -> Optional[str]:\n",
    "    # Find the problem header by regex, independent of project name\n",
    "    header = soup.find(\n",
    "        lambda t: isinstance(t, Tag)\n",
    "        and t.name in (\"h2\", \"h3\")\n",
    "        and regex.search(t.get_text(strip=True) or \"\")\n",
    "    )\n",
    "    if not header:\n",
    "        return None\n",
    "\n",
    "    # Find the next div whose class contains 'ProjectListingContent'\n",
    "    plc_div = None\n",
    "    for el in header.next_elements:\n",
    "        if isinstance(el, Tag) and el.name == \"div\":\n",
    "            classes = \" \".join(el.get(\"class\", []))\n",
    "            if \"ProjectListingContent\" in classes:\n",
    "                plc_div = el\n",
    "                break\n",
    "        # stop if a new major section header is encountered\n",
    "        if isinstance(el, Tag) and el.name in (\"h2\", \"h3\") and el is not header:\n",
    "            break\n",
    "\n",
    "    if not plc_div:\n",
    "        return None\n",
    "\n",
    "    # Extract readable text from the ProjectListingContent block\n",
    "    # Prefer paragraphs and list items; fall back to full text if needed\n",
    "    lines: List[str] = []\n",
    "    for br in plc_div.find_all([\"br\"]):\n",
    "        br.replace_with(\"\\n\")\n",
    "    for el in plc_div.descendants:\n",
    "        if isinstance(el, Tag):\n",
    "            if el.name == \"p\":\n",
    "                txt = el.get_text(\" \", strip=True)\n",
    "                if txt:\n",
    "                    lines.append(txt)\n",
    "            elif el.name == \"li\":\n",
    "                li_txt = el.get_text(\" \", strip=True)\n",
    "                if li_txt:\n",
    "                    lines.append(li_txt)\n",
    "\n",
    "    text = \"\\n\\n\".join([l for l in lines if l]).strip()\n",
    "    return text or plc_div.get_text(\" \", strip=True) or None\n",
    "\n",
    "\n",
    "def extract_technologies(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"Return a list of technology names from Devfolio project pages.\n",
    "    Targets divs whose class contains 'ProjectTechCard__ProjectTechChip' and\n",
    "    collects their <p> text content. Deduplicates while preserving order.\n",
    "    \"\"\"\n",
    "    technologies: List[str] = []\n",
    "    seen = set()\n",
    "\n",
    "    def has_chip_class(tag: Tag) -> bool:\n",
    "        classes = tag.get(\"class\", [])\n",
    "        if not classes:\n",
    "            return False\n",
    "        joined = \" \".join(classes)\n",
    "        return \"ProjectTechCard__ProjectTechChip\" in joined or \"ProjectTech\" in joined\n",
    "\n",
    "    for chip_div in soup.find_all(lambda t: isinstance(t, Tag) and t.name == \"div\" and has_chip_class(t)):\n",
    "        p = chip_div.find(\"p\")\n",
    "        if p:\n",
    "            name = p.get_text(\" \", strip=True)\n",
    "            if name and name not in seen:\n",
    "                seen.add(name)\n",
    "                technologies.append(name)\n",
    "\n",
    "    # Fallback: search any <p> whose parent div class includes 'ProjectTech'\n",
    "    if not technologies:\n",
    "        for p in soup.find_all(\"p\"):\n",
    "            parent = p.parent\n",
    "            if isinstance(parent, Tag):\n",
    "                classes = \" \".join(parent.get(\"class\", []))\n",
    "                if \"ProjectTech\" in classes:\n",
    "                    name = p.get_text(\" \", strip=True)\n",
    "                    if name and name not in seen:\n",
    "                        seen.add(name)\n",
    "                        technologies.append(name)\n",
    "\n",
    "    return technologies\n",
    "\n",
    "def scrape_devfolio_project(url: str, timeout: int = 20) -> Dict[str, Any]:\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                      \"(KHTML, like Gecko) Chrome/124.0 Safari/537.36\"\n",
    "    }\n",
    "    resp = requests.get(url, headers=headers, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    def text_or_none(el) -> Optional[str]:\n",
    "        return el.get_text(strip=True) if el else None\n",
    "\n",
    "    # Basic fields\n",
    "    title = text_or_none(soup.find([\"h1\"]))\n",
    "    subtitle = text_or_none(soup.find([\"h2\"]))\n",
    "    \n",
    "\n",
    "    # Quadratic voting / stats\n",
    "    page_text = soup.get_text(\" \", strip=True)\n",
    "    matching_amount = None\n",
    "    votes = None\n",
    "    quadratic_votes = None\n",
    "\n",
    "    m = re.search(r\"\\$([0-9][0-9,]*)\\s*Matching Amount\", page_text, flags=re.I)\n",
    "    if m:\n",
    "        matching_amount = m.group(1).replace(\",\", \"\")\n",
    "        try:\n",
    "            matching_amount = int(matching_amount)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    m = re.search(r\"(\\d+)\\s*Votes\", page_text, flags=re.I)\n",
    "    if m:\n",
    "        votes = int(m.group(1))\n",
    "\n",
    "    m = re.search(r\"([0-9]+(?:\\.[0-9]+)?)\\s*Quadratic Votes\", page_text, flags=re.I)\n",
    "    if m:\n",
    "        try:\n",
    "            quadratic_votes = float(m.group(1))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # Built at / dates\n",
    "    built_at = None\n",
    "    m = re.search(r\"Built at\\s+([A-Za-z0-9 ]+)\", page_text, flags=re.I)\n",
    "    if m:\n",
    "        built_at = m.group(1).strip()\n",
    "\n",
    "    created_on = None\n",
    "    m = re.search(r\"Created on\\s+([0-9]{1,2}(?:st|nd|rd|th)?\\s+[A-Za-z]+\\s+[0-9]{4})\", page_text, flags=re.I)\n",
    "    if m:\n",
    "        created_on = m.group(1)\n",
    "\n",
    "    last_edited = None\n",
    "    m = re.search(r\"Last Edited\\s+([0-9]{1,2}(?:st|nd|rd|th)?\\s+[A-Za-z]+\\s+[0-9]{4})\", page_text, flags=re.I)\n",
    "    if m:\n",
    "        last_edited = m.group(1)\n",
    "    github_link = None\n",
    "    m = re.search(r'github\\.com/[^/\\s]+/[^/\\s]+', str(soup), flags=re.I)\n",
    "    if m:\n",
    "        github_link = \"https://\" + m.group(0)\n",
    "\n",
    "    problem_statement = extract_problem_section(soup,_PROBLEM_HEADING_RE)\n",
    "    challenges=extract_problem_section(soup,_CHALLENGE_RE)\n",
    "    technologies=extract_technologies(soup)\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"subtitle\": subtitle,\n",
    "        \"technologies\": technologies,\n",
    "        \"matching_amount_usd\": matching_amount,\n",
    "        \"votes\": votes,\n",
    "        \"quadratic_votes\": quadratic_votes,\n",
    "        \"built_at\": built_at,\n",
    "        \"created_on\": created_on,\n",
    "        \"last_edited\": last_edited,\n",
    "        \"problem_statement\": problem_statement,\n",
    "        \"challenges\":challenges,\n",
    "        \"github_link\":github_link\n",
    "    }\n",
    "\n",
    "# Example\n",
    "data = scrape_devfolio_project(\"https://devfolio.co/projects/empoweria-e0ee\")\n",
    "print(json.dumps(data, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
