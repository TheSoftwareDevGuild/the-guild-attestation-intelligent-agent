{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd96518",
   "metadata": {},
   "source": [
    "## Save USDT Transfers to json (run hypersync example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e7a96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import hypersync\n",
    "from hypersync import BlockField, TransactionField, LogField, ClientConfig\n",
    "import json\n",
    "\n",
    "async def main():\n",
    "    client = hypersync.HypersyncClient(ClientConfig())\n",
    "\n",
    "    # The query to run\n",
    "    query = hypersync.Query(\n",
    "        # start from block 0 and go to the end of the chain (we don't specify a toBlock).\n",
    "        from_block=0,\n",
    "        # The logs we want. We will also automatically get transactions and blocks relating to these logs (the query implicitly joins them).\n",
    "        logs=[\n",
    "            hypersync.LogSelection(\n",
    "                # We want All ERC20 transfers so no address filter and only a filter for the first topic\n",
    "                topics=[\n",
    "                    [\n",
    "                        \"0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef\"\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        # Select the fields we are interested in, notice topics are selected as topic0,1,2,3\n",
    "        field_selection=hypersync.FieldSelection(\n",
    "            block=[BlockField.NUMBER, BlockField.TIMESTAMP, BlockField.HASH],\n",
    "            log=[\n",
    "                LogField.LOG_INDEX,\n",
    "                LogField.TRANSACTION_INDEX,\n",
    "                LogField.TRANSACTION_HASH,\n",
    "                LogField.DATA,\n",
    "                LogField.ADDRESS,\n",
    "                LogField.TOPIC0,\n",
    "                LogField.TOPIC1,\n",
    "                LogField.TOPIC2,\n",
    "                LogField.TOPIC3,\n",
    "            ],\n",
    "            transaction=[\n",
    "                TransactionField.BLOCK_NUMBER,\n",
    "                TransactionField.TRANSACTION_INDEX,\n",
    "                TransactionField.HASH,\n",
    "                TransactionField.FROM,\n",
    "                TransactionField.TO,\n",
    "                TransactionField.VALUE,\n",
    "                TransactionField.INPUT,\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # start the stream\n",
    "    receiver = await client.stream(query, hypersync.StreamConfig())\n",
    "\n",
    "    decoder = hypersync.Decoder(\n",
    "        [\"Transfer(address indexed from, address indexed to, uint256 value)\"]\n",
    "    )\n",
    "\n",
    "    # Let's count total volume, it is meaningless because of currency differences but good as an example.\n",
    "    total_volume = 0\n",
    "    all_logs = []  # Store all logs for JSON export\n",
    "    continue_loop = True\n",
    "    \n",
    "    while continue_loop:\n",
    "        res = await receiver.recv()\n",
    "        # exit if the stream finished\n",
    "        if res is None:\n",
    "            break\n",
    "\n",
    "        # Decode the log on a background thread so we don't block the event loop.\n",
    "        # Can also use decoder.decode_logs_sync if it is more convenient.\n",
    "        decoded_logs = await decoder.decode_logs(res.data.logs)\n",
    "        \n",
    "        for log in decoded_logs:\n",
    "            # skip invalid logs\n",
    "            if log is None:\n",
    "                continue\n",
    "            \n",
    "            print(log)\n",
    "            print(dir(log.body[0]))\n",
    "            \n",
    "            # Convert DecodedEvent to dictionary for JSON serialization\n",
    "            if hasattr(log, 'to_dict'):\n",
    "                log_dict = log.to_dict()\n",
    "            elif hasattr(log, '__dict__'):\n",
    "                log_dict = log.__dict__\n",
    "            else:\n",
    "                # Fallback: convert to string representation\n",
    "                log_dict = {\n",
    "                    'from': str(log.indexed[0].val) if hasattr(log.indexed[0], 'val') else None,\n",
    "                    'to': str(log.indexed[1].val) if hasattr(log.indexed[1], 'val') else None,\n",
    "                    'value': str(log.body[0].val) if hasattr(log.body[0], 'val') else None,\n",
    "                    'raw_log': str(log)\n",
    "                }\n",
    "            \n",
    "            all_logs.append(log_dict)\n",
    "            total_volume += log.body[0].val\n",
    "        \n",
    "        total_blocks = res.next_block - query.from_block\n",
    "        print(f\"reached block {res.next_block}\")\n",
    "        print(f\"total volume was {total_volume} in {total_blocks} blocks\")\n",
    "        \n",
    "        # Save to JSON file after processing each batch\n",
    "        with open('ERC20-transfer-logs.json', 'w') as f:\n",
    "            json.dump(all_logs, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"Saved {len(all_logs)} logs to logs.json\")\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
